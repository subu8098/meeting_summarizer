Coding: 

I along with Mr. Amithesh worked with “meeting summarizing AI agent”, since we learned few necessary things yesterday, we start to write code with the approaches I had. 

We tried with the 1st approach (speech recognizer for both listen and transcribe) I downloaded need libraries, tools and models. 

It takes some time to download the dependencies like speech recognizer, Ollama, llama (LLM), torch and a few more things. 

Test and debug: 

The code itself has lots of logic problems, every time run and debug the problem 

 

I also downloaded the gemma (LLM) to test the efficiency over llama and yes it had more clear summarizing over that. 

 

Approach 2: 

Then I tried the 2nd approach, using speech recognizer for listening and whisper from open AI for transcribe. 

It took more time to download whisper and its needed dependencies. 

While working with this approach I faced an issue with ffmpeg, it is a main thing to transcribe audio to text. 

After fixing the issues I got the output from this approach, which is more accurate and effective than the 1st approach. 

 

 
